{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduktion\n",
    "Vi vil ved hjælp af diverse pakker udviklet til python kigge på vores datasæt, hvor vi vil arbejde med demografisk filtrering, se på fordele og ulemper med demografisk filtrering dernæst vil vi arbejde med indhold baseret filtrering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credits = pd.read_csv(\"data/tmdb_5000_credits.csv\")\n",
    "df_movies = pd.read_csv(\"data/tmdb_5000_movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efter at vi har læst vores data filer, vil vi gerne se nærmerer på dem, for at skabe en generel forståelse for datasættets indhold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det ses at begge datasæt indeholder en variabel kaldet \"id\" og \"movie_id\". Dette er en reference variabel der er ens for begge datasæt, det betyder, at en vilkårlig film med eksempelvis titel \"Avatar\" har \"id\" = 19995 og det samme er gældende for \"movie_id\" med samme titel, har også værdien 19995.\n",
    "\n",
    "Vi vil gerne samle vores data, så vi tilføjer \"crew\" og \"cast\" til vores film datasæt og dette kan vi gøre på id'et, da de er ens for begge datasæt. Inden vi gør det, fjerner vi variablen \"title\" fra vores \"credits\" datasæt, da titlen er redundant og dermed allerede eksistere i vores film datasæt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_credits.drop('title', axis=1, inplace=True)\n",
    "df_credits.columns = ['id', 'cast', 'crew']\n",
    "main_df = df_movies.merge(df_credits, on=\"id\")\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info=pd.DataFrame(main_df.dtypes).T.rename(index={0:'column type'})\n",
    "df_info=df_info.append(pd.DataFrame(main_df.isnull().sum()).T.rename(index={0:'null values'}))\n",
    "df_info=df_info.append(pd.DataFrame(main_df.isnull().sum()/main_df.shape[0]*100).T.rename(index={0:'null values (%)'}))\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_in_percentages():\n",
    "    non_nan = []\n",
    "    [non_nan.append(val) for val in df_info.loc['null values'] if val == 0]\n",
    "    non_nan = [1 - len(non_nan) / len(df_info.loc['null values (%)']), len(non_nan) / len(df_info.loc['null values'])]\n",
    "    return non_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make figure and assign axis objects\n",
    "fig = plt.figure(figsize=(9, 5.0625)) #5.0625\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "fig.subplots_adjust(wspace=0)\n",
    "\n",
    "# pie chart parameters\n",
    "ratios = nan_in_percentages()\n",
    "labels = ['NaN values', 'Non NaN values']\n",
    "explode = [0.1, 0]\n",
    "# rotate so that first wedge is split by the x-axis\n",
    "angle = -180 * ratios[0]\n",
    "ax1.pie(ratios, autopct='%1.1f%%', startangle=angle,\n",
    "        labels=labels, explode=explode)\n",
    "\n",
    "# bar chart parameters\n",
    "\n",
    "xpos = 0\n",
    "bottom = 0\n",
    "ratios = sorted([val/sum(df_info.loc['null values'].values) for val in df_info.loc['null values'] if val != 0], reverse=False)\n",
    "width = .2\n",
    "colors = [[.1, .3, .1], [.1, .3, .9], [.1, .3, .7], [.1, .3, .3], [.1, .3, .5]]\n",
    "\n",
    "for j in range(len(ratios)):\n",
    "    height = ratios[j]\n",
    "    ax2.bar(xpos, height, width, bottom=bottom, color=colors[j])\n",
    "    ypos = bottom + ax2.patches[j].get_height() / 2\n",
    "    bottom += height\n",
    "    ax2.text(xpos, ypos, \"%d%%\" % (ax2.patches[j].get_height() * 100),\n",
    "             ha='center')\n",
    "\n",
    "ax2.set_title('Has NaN values')\n",
    "ax2.legend(([df_info.loc['null values'].sort_values(ascending=True).index[col] for col, val in enumerate(df_info.loc['null values'].sort_values(ascending=True)) if val != 0]))\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(-2.5 * width, 2.5 * width)\n",
    "\n",
    "# use ConnectionPatch to draw lines between the two plots\n",
    "# get the wedge data\n",
    "theta1, theta2 = ax1.patches[0].theta1, ax1.patches[0].theta2\n",
    "center, r = ax1.patches[0].center, ax1.patches[0].r\n",
    "bar_height = sum([item.get_height() for item in ax2.patches])\n",
    "\n",
    "# draw top connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta2) + center[0]\n",
    "y = np.sin(np.pi / 180 * theta2) + center[1]\n",
    "con = ConnectionPatch(xyA=(- width / 2, bar_height), xyB=(x, y),\n",
    "                      coordsA=\"data\", coordsB=\"data\", axesA=ax2, axesB=ax1)\n",
    "con.set_color([0, 0, 0])\n",
    "con.set_linewidth(4)\n",
    "ax2.add_artist(con)\n",
    "\n",
    "# draw bottom connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta1) + center[0]\n",
    "y = np.sin(np.pi / 180 * theta1) + center[1]\n",
    "con = ConnectionPatch(xyA=(- width / 2, 0), xyB=(x, y), coordsA=\"data\",\n",
    "                      coordsB=\"data\", axesA=ax2, axesB=ax1)\n",
    "con.set_color([0, 0, 0])\n",
    "ax2.add_artist(con)\n",
    "con.set_linewidth(4)\n",
    "\n",
    "#plt.savefig('testpiepig_pandas_900dpi.svg', dpi=900)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det ses på grafen ovenfor, at det 77,3% af alle variabler i vores datasæt indeholder data, mens 22,7% ikke gør, men ud af de 22,7% er 78% af dem data der mangler på variablen \"homepage\". <br>\n",
    "\"homepage\" variablen er sammentidigt en variabel vi ikke kan bruge til noget i forhold til det initierende problem, som er: Der ønskes film anbefalinger baseret på det givne datasæt. <br>\n",
    "21% er variablen \"tagline\" der indeholder noget beskrivende om filmen handling - Denne variabel vil være oplagt at lave TF-IDF på, men da den indeholder så mange NaN værdier på man stærkt overveje, om variablen stadig bør anvendes. <br>\n",
    "\"realease_date\", \"runtime\", \"overview\" udgør hver især mindre end 1% og derfor er der tale om få NaN værdier i disse variabler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 film"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 film baseret på den gennemsnitlige bedømmelse\n",
    "Nu når alt data er samlet ét datasæt, er vi nu interesseret, at give nogle film anbefalinger ud fra det data vi er givet.\n",
    "Dette kan lade sig gøre ved at se på variablen vote_average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[[\"title\", \"release_date\", \"vote_average\", \"vote_count\"]].sort_values(\"vote_average\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det ses på tabellen ovenfor de 10 bedst bedømte film.<br>\n",
    "Dette er ikke særlig given, da vi kan se nogle af de bedst rated film kun har 1 til 2 stemmer.\n",
    "\n",
    "### Top 10 film baseret på det vægtede gennemsnit\n",
    "\n",
    "Derfor vil vi lave en ny rating variabel der giver filmene en ny rating der også betragter antal stemmer den enkelte film har fået. Dette kan gøres ved hjælp af noget kaldt \"Weighted Rating\", der kan skrives som:<br>\n",
    "Weighted Rating (WR)=(v/(v+m))R+(m/(v+m))C<br>\n",
    "hvor:<br>\n",
    "R = gennemsnits rating for filmen = \"vote_average\".<br>\n",
    "v = antallet af stemmer for filmen = \"vote_count\".<br>\n",
    "m = minimum antal stemmer krævet, for at være en \"gyldig\" film.<br>\n",
    "C = gennemsnits rating for alle film.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = main_df['vote_count'].quantile(0.9)\n",
    "C = main_df[\"vote_average\"].mean()\n",
    "\n",
    "main_df[\"WR\"] = (main_df[\"vote_count\"]/(main_df[\"vote_count\"] + m)) * main_df[\"vote_average\"] + (m/(main_df[\"vote_count\"] + m)) * C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Det nye resultat\n",
    "Der sorteres nu på de 10 bedste film baseret på vores nye variabel kaldet \"WR\" og det ses nu, at de film anbefalet baseret på WR er langt fra de 10 film vi fik anbefalet tidligere, dette skyldes at der vægtes på antallet af stemmer en film har modtaget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_df[['title', \"release_date\", 'vote_average', 'vote_count', 'WR']].sort_values(\"WR\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populære film"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan nu give en komfortabel anbefaling af film baseret på deres bedømmelse, men nogle af de film er af ældre dato, hvilket leder os frem til spørgsmåle - hvad der er populært lige nu og her?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_movies = main_df.sort_values(\"popularity\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "\n",
    "plt.bar(popular_movies['title'].head(10),popular_movies['popularity'].head(10), align='center', color='#211a52')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.title(\"Top 10 Popular Movies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skaleret Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har nu de film der trender og de film, der har en høj rating. <br>\n",
    "Hvis disse to variabler ønskes kombineret og vægtes ligeligt. <br>\n",
    "Det vil resulterer i at WR og popularity vil vægte 50% hver især."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = MinMaxScaler().fit_transform(main_df[['popularity','WR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scaled_score(x):\n",
    "    return [(0.5 * ss[i][0]) + (0.5 * ss[i][-1]) for i in range(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df['scaled_score'] = scaled_score(len(main_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.bar(main_df.sort_values(\"scaled_score\", ascending=False)['title'].head(10), \n",
    "         main_df.sort_values(\"scaled_score\", ascending=False)['scaled_score'].head(10), \n",
    "         align='center', color='#211a52')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Rating & Popularity Combined (Hybrid Score)\")\n",
    "plt.title(\"Top 10 Movies with Scaled Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "Selvom vi har nogle løsninger i form af anbefalinger er løsningerne meget overfaldiske og generelle, da de ikke tager højde for den enkelte brugers preferencer. <br>\n",
    "Dette vil vi tage højde for nu, ved at indtaste eksempelvis sin yndlingsfilm og ud fra den film finde film der minder om den indtastede film.<br>\n",
    "Da mange af variablerne indeholder tekst værdier, er det nødvendigt at lave tekst om til noget vi kan regne på eksempelvis med tal værdier. Dette kan lade sig gøre ved hjælp af TD-IDF. <br>\n",
    "Der eksisterer dog nogle variabler i datasættet der har værdier vi gerne vil rense, for unødvendige værdier og omskrive til nogle håndgribelige værdier, så vi kan udfører vores tekst analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the director's name from the crew feature. If director is not listed, return NaN\n",
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the \"names\" from genre, cast, crew, keywords, productions_companies and spoken_languages.\n",
    "# If there are no \"names\" return an empty list\n",
    "def get_list(x):\n",
    "    return [i['name'] for i in x] if isinstance(x, list) else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['cast', 'crew', 'keywords', 'genres', 'production_companies', 'production_countries', 'spoken_languages']\n",
    "for feature in features:\n",
    "    cleaned_df[feature] = cleaned_df[feature].apply(literal_eval)\n",
    "\n",
    "# Define new director, cast, genres and keywords features that are in a suitable form.\n",
    "cleaned_df['director'] = cleaned_df['crew'].apply(get_director)\n",
    "\n",
    "for feature in features:\n",
    "    cleaned_df[feature] = cleaned_df[feature].apply(get_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sådan ser de nye variabler ud efter noget rensning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleaned_df[features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafisk oversigt\n",
    "Her gives et generelt overblik over de forskellige værdier i datasættet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(main_df.sort_values(\"budget\", ascending=False)['title'].head(10), \n",
    "        main_df.sort_values(\"budget\", ascending=False)['budget'].head(10), \n",
    "        align='center', color='#211a52')\n",
    "plt.xlabel(\"Budget in $\")\n",
    "plt.ticklabel_format(axis='y', style='plain')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 10 Movies with Highest Budget\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(main_df.sort_values(\"revenue\", ascending=False)['title'].head(10), \n",
    "         main_df.sort_values(\"revenue\", ascending=False)['revenue'].head(10), \n",
    "         align='center', color='#211a52')\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Revenue in $\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ticklabel_format(axis='y', style='plain')\n",
    "#plt.xaxis.get_offset_text().set_visible(False)\n",
    "plt.title(\"Top 10 Movies with Highest Revenue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(main_df.sort_values(\"runtime\", ascending=False)['title'].head(10), \n",
    "         main_df.sort_values(\"runtime\", ascending=False)['runtime'].head(10), \n",
    "         align='center', color='#211a52')\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Watch time in minutes\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ticklabel_format(axis='y', style='plain')\n",
    "#plt.xaxis.get_offset_text().set_visible(False)\n",
    "plt.title(\"Top 10 Longest to Watch Movies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_bar_plot(data, title, len_return_val):\n",
    "    plt.subplots(figsize=(12,10))\n",
    "    \n",
    "    data_list = []\n",
    "    for i in data:\n",
    "        if isinstance(i, str):\n",
    "            if i != '':\n",
    "                data_list.append(i)\n",
    "        else:\n",
    "            data_list.extend(i)\n",
    "\n",
    "    ax = pd.Series(data_list).value_counts()[:len_return_val].sort_values(ascending=False).plot.bar(width=0.9,color='#211a52')\n",
    "    \n",
    "    #for i, v in enumerate(pd.Series(data_list).value_counts()[:len_return_val].sort_values(ascending=False).values): \n",
    "        #ax.text(.8, i, v, fontsize=12, color='white', weight='bold')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_bar_plot(cleaned_df['genres'], 'Top Genres', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vertical_bar_plot(cleaned_df['cast'], 'Actors with most appearance', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_bar_plot(cleaned_df['production_companies'], 'Production Companies with most associations', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_bar_plot(cleaned_df['director'].fillna(''), 'Directors with most movies', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_bar_plot(cleaned_df['keywords'], 'Most used keywords', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_bar_plot(cleaned_df['production_countries'], 'Production Countries with most movies', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_bar_plot(cleaned_df['spoken_languages'], 'Most spoken language', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = []\n",
    "for date in cleaned_df['release_date'].fillna('').str.split('-'):\n",
    "    year_list.append(date[0])\n",
    "vertical_bar_plot(year_list, 'Year with most released movies', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.drop(columns=['homepage', 'original_language', 'tagline', 'original_title', 'runtime', 'spoken_languages', 'vote_average', 'vote_count', 'status', 'production_countries', 'crew', 'budget', 'revenue', 'release_date', 'genres', 'production_companies'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF og Content Based Filtering\n",
    "Det er nu tid til at påbegynde den del, der anbefaler eksempelvis 10 film baseret på sin yndlingsfilm eller en hvilken som helst anden given film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It is required to install stopwords, before running this code. This is done by:\n",
    "# >conda install -c anaconda nltk\n",
    "# >python\n",
    "# >>>import nltk\n",
    "# >>>nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(',', '―', ';', '!', '?', '.', '(', ')', '$', '#', '+', ':', '...', ' ', '', \"he's\",\"who's\", \"it's\", 'hes', 'shes', 'whos', '--', '–')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN with an empty string\n",
    "#cleaned_df['overview'] = cleaned_df['overview'].fillna('') # Or drop NA?\n",
    "#cleaned_df = cleaned_df[cleaned_df['overview'].notna()]\n",
    "cleaned_df.drop(cleaned_df.loc[cleaned_df['overview'] == ' '].index, inplace=True)\n",
    "cleaned_df.dropna(subset=['overview'], inplace=True)\n",
    "cleaned_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overview_cleaning funkltionen\n",
    "Denne funktion rengører vores film beskrivelse ved hjælp af stopwords og en ekstra filtrering i form af special tegn og andet.\n",
    "<br>\n",
    "<br>\n",
    "OBS - Det er vigtigt at påpege, at tf-idf ikke kan genkende synonymer eller forstår stavefejl eller sætningen i sig selv, men kigger kun på ordet eller tekst strengen i sig selv. Dette vil resulterer i at eksempelvis at peter og peters ikke er det samme, ligesom at peters og peter's heller ikke har samme betydning.\n",
    "Da vores liste af unikke ord efter filtrering af stopwords og alt andet indeholder stadig 24067 ord og der er med en statistisk sikkerhed ord der burde være \"ens\" men ikke er, da ordet har en apostrof eller andet der differentierer den fra det originale ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview_cleaning(x):\n",
    "    x = x.lower().strip('[]').replace('\"','').replace('!','').replace('?','').replace(';','').replace('--','').replace('.','').replace(',','').replace('(','').replace(')','').replace(\"'\",'').replace('\"','').replace(':','').replace('“','').split()\n",
    "    return [w for w in x if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['overview_cleaned'] = cleaned_df['overview'].apply(lambda x: overview_cleaning(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unikke ord\n",
    "Her oprettes variablen der indheholder alle unikke ord og vil blive anvendt flere gange senere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "for sentence in cleaned_df['overview_cleaned']:\n",
    "    bigrams += ([w + ' ' + sentence[i+1] for i, w in enumerate(sentence) if i < len(sentence)-1])\n",
    "\n",
    "bigrams = list(set(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigrams = []\n",
    "for token in cleaned_df['overview_cleaned']:\n",
    "    unigrams.extend(token)\n",
    "unigrams = ([word for word in unigrams])\n",
    "unigrams = list(set(unigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "Her beregnes Term Frequency, funktionen tf_calculator returnerer en dictionary med de ord der er anvendt for en vilkårlig film som key og antal gange ét ord indgår i beskrivelsen for den vilkårlige film og dette divideres med det totale antal af ord i beskrivelsen og dette er vores value for ordet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_calculator(x, is_unigram):\n",
    "    bow_dict = {}\n",
    "    if is_unigram:\n",
    "        bow_dict = dict.fromkeys(x, 0)\n",
    "        for w in x:\n",
    "            bow_dict[w] += 1\n",
    "        for key in bow_dict:\n",
    "            bow_dict[key] = bow_dict[key] / len(x)\n",
    "    else:\n",
    "        bow_dict = dict.fromkeys([w + ' ' + x[i+1] for i, w in enumerate(x) if i < len(x)-1], 0)\n",
    "        for i in range(len(x)-1):\n",
    "            bow_dict[x[i] + ' ' + x[i+1]] += 1\n",
    "        for key in bow_dict:\n",
    "            bow_dict[key] = bow_dict[key] / x.count(key.split()[0])\n",
    "    return(bow_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_bigrams = cleaned_df['overview_cleaned'].apply(lambda x: tf_calculator(x, is_unigram=False))\n",
    "tf_unigrams = cleaned_df['overview_cleaned'].apply(lambda x: tf_calculator(x, is_unigram=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF\n",
    "Her beregnes Inverse Document Frequency. funktionen idf_calculator returnerer en dictionary med unikke ord som keys og som value er det antal af film divideret med antallet af gange ordet x forekommer - Hvis ordet x forekommer i alle film vil værdien være 4803, da vi har i alt 4803 film og dette er også tilfældet selvom ordet x forekommer mere end 1 gang i flere film, da dette ikke tages højde for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bør kun eksekveres 1 gang, hvis \"data/idf_dict.pkl\" eller \"data/idf_dict.pkl_unigram\" ikke eksisterer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N antal gange unigram optræder i film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_dict = dict.fromkeys(unigrams, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_idf_dict = {k: v for k, v in idf_dict.items() if v < 1}\n",
    "#print(len(temp_idf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word in temp_idf_dict:\n",
    "#    is_word_existing = True\n",
    "#    for movie in cleaned_df['overview_cleaned']:\n",
    "#        movie = ([w for w in movie])\n",
    "#        if word in movie:\n",
    "#            idf_dict[word] += 1\n",
    "#            is_word_existing = False\n",
    "#    if is_word_existing:\n",
    "#        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"data/idf_dict_unigram.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(idf_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N antal gange bigram optræder i film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_dict = dict.fromkeys(bigrams, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_idf_dict = {k: v for k, v in idf_dict.items() if v < 1}\n",
    "#print(len(temp_idf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word in temp_idf_dict:\n",
    "#    is_word_existing = True\n",
    "#    for movie in cleaned_df['overview_cleaned']:\n",
    "#        movie = ([w + ' ' + movie[i+1] for i, w in enumerate(movie) if i < len(movie)-1])\n",
    "#        if word in movie:\n",
    "#            idf_dict[word] += 1\n",
    "#            is_word_existing = False\n",
    "#    if is_word_existing:\n",
    "#        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"data/idf_dict.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(idf_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/idf_dict.pkl\", \"rb\") as file:\n",
    "    bigrams_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/idf_dict_unigram.pkl\", \"rb\") as file:\n",
    "    unigrams_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(bigrams_dict.items()), key=lambda x: x[1], reverse=True)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(unigrams_dict.items()), key=lambda x: x[1], reverse=True)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_calculator(ngram_dict):\n",
    "    for key in ngram_dict:\n",
    "        ngram_dict[key] = math.log10(len(cleaned_df)/ngram_dict[key])\n",
    "    return ngram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_bigrams = idf_calculator(bigrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_unigrams = idf_calculator(unigrams_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "funktionen returnerer en liste af dictionaries, hvor værdierne i disse dictionaries ganges sammen med den tilsvarende idf værdi - altså idf værdien for ordet \"peter\" ganges med tf værdien for samme ord, altså \"peter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfidf_calculator(tf, idf):\n",
    "    for key in tf:\n",
    "        if key in idf:\n",
    "            tf[key] = tf[key] * idf[key]\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_bigrams = tf_bigrams.apply(lambda x: tfidf_calculator(x, idf_bigrams))\n",
    "tfidf_unigrams = tf_unigrams.apply(lambda x: tfidf_calculator(x, idf_unigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kombinering af unigrams og bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_calculator():\n",
    "    for i in range(len(cleaned_df)):\n",
    "        for uni in tfidf_unigrams[i]:\n",
    "            for bi in tfidf_bigrams[i]:\n",
    "                if uni == bi.split()[0]:\n",
    "                    tfidf_bigrams[i][bi] = tfidf_bigrams[i][bi] * tfidf_unigrams[i][uni]\n",
    "    return tfidf_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_bigrams = bigram_calculator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matricen\n",
    "Funktionen tfidf_matrix returnerer en matrice, der går igennem alle unikke ord og giver tilføjer værdien 0, hvis ordet ikke eksisterer i, beskrivelsen af en given film ellers tilføjes TF-IDF værdien, hvis ordet eksisterer i beskrivelsen til filmen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix(tfidf_dict, ngrams):\n",
    "    return {i: tfidf_dict[ngram] for i, ngram in enumerate(ngrams) if ngram in tfidf_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_df['bigram_matrix'] = tfidf_bigrams.apply(lambda x: tfidf_matrix(x, bigrams))\n",
    "#cleaned_df['unigram_matrix'] = tfidf_unigrams.apply(lambda x: tfidf_matrix(x, unigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_df.to_pickle(\"data/tfidf_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nedenstående kode kan køres seperat (OBS kør imports først)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pd.read_pickle(\"data/tfidf_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Simularity\n",
    "Der er umiddelbart to metoder vi kan anvende, da vi har lister af værdier, den metode, hvor numpy anvendes, siges at være den \"hurtigeste\" i form af runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector_of_interest, comparison_vector):\n",
    "    cos_sim = 0\n",
    "    for key in vector_of_interest:\n",
    "        if key in comparison_vector:\n",
    "            cos_sim += (vector_of_interest[key] * comparison_vector[key])\n",
    "    length_voi = [vector_of_interest[i]**2 for i in vector_of_interest]\n",
    "    length_cv = [comparison_vector[i]**2 for i in comparison_vector]\n",
    "    \n",
    "    return cos_sim / (math.sqrt(sum(length_voi)) * math.sqrt(sum(length_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, matrix):\n",
    "    main_movie_vector = cleaned_df.loc[cleaned_df['title'] ==  title][matrix].values[0]\n",
    "    cos_sim = [cosine_similarity(main_movie_vector, cleaned_df[matrix][i]) for i in range(len(cleaned_df) -1)]\n",
    "    \n",
    "    return [cleaned_df.iloc[i]['title'] for i in pd.Series(cos_sim).nlargest(11).index[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = get_recommendations('Star Wars', 'bigram_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Empire Strikes Back\n",
      "Return of the Jedi\n",
      "Roadside\n",
      "The Lost Boys\n",
      "Star Wars: Episode III - Revenge of the Sith\n",
      "Gangs of New York\n",
      "The Last Dragon\n",
      "Haywire\n",
      "The Legend of Hercules\n",
      "Alpha and Omega\n"
     ]
    }
   ],
   "source": [
    "print(*top_movies, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies = get_recommendations('Star Wars', 'unigram_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Empire Strikes Back\n",
      "Return of the Jedi\n",
      "Shanghai Noon\n",
      "Arbitrage\n",
      "The Princess Diaries 2: Royal Engagement\n",
      "Prince of Persia: The Sands of Time\n",
      "History of the World: Part I\n",
      "The Princess Bride\n",
      "Star Wars: Episode III - Revenge of the Sith\n",
      "Mirror Mirror\n"
     ]
    }
   ],
   "source": [
    "print(*top_movies, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
